---
title: "Reproducible road safety research: an exploration of the shifting spatial and temporal distribution of car-pedestrian crashes"
author:
- Dr Robin Lovelace --- University of Leeds, Consumer Data Research Centre (CDRC) and Institute for Transport Studies (ITS) and Leeds Institute for Data Analytics (LIDA)
- Dr Layik Hama --- University of Leeds, Consumer Data Research Centre (CDRC) and Leeds Institute for Data Analytics (LIDA)
date: '`r Sys.Date()`'
output:
  github_document
  # pdf_document:
  #   fig_caption: yes
  #   number_sections: true
bibliography: references.bib
---



# Summary {-}

This paper demonstrates a reproducible analysis workflow for downloading, formatting and analysing road crash data.
Building on the recently published **stats19** R package, the paper contains code that downloads 691,641 crash incidents, which are filtered-down to 78,448 car-pedestrian crashes.
The dataset reveals variability in crash characteristics depending on the age of the person hurt, speed limits and location (urban-rural in the first instance).
More importantly, the beginnings of evaluation metrics are shown using high-level geographic aggregation, raising many further questions and directions for future research using open road crash data.

<!-- Authors are requested to keep to the word limit of 1500 words. The word limit includes the main body of the abstract and everything within (including captions etc.,) and the references. Not included in the word count is the title, author list, date, summary, keywords and author biographies -->

**Keywords:** geographic analysis, road safety, reproducibility

# Introduction

This paper is motivated by two high-level policy and academic objectives, which intersect.
The policy objective is to make decisions be made with reference to and understanding of high quality evidence that is based on the 'best available data'.
The academic object is to ensure that research findings can be reproduced, to ensure scientific falsifiability and to encourage collaboration and cooperation between researchers, rather than competition.
These two objectives intersect because without them it is difficult to generate high quality evidence, that can be externally verified, without reproducible methods.
Conversely, an academic environment that is conducive to collaboration and not competition requires a government that supports open science, "the transparent and accessible knowledge [and methods] shared and developed through collaborative networks" [@vicente-saez_open_2018].

This context is relevant to many fields of research that have practical and policy implications, and road safety research is no exceptions, as its findings often have direct policy implications and can be highly emotive, raising questions about the divide between research and policy [@elvik_handbook_2009]:

> Can science and politics be kept apart in such a
highly applied field of research? Where is the dividing line between science and politics
in road safety?

More specifically, how can road safety research become more reproducible?
This would clearly have advantages for many stakeholders: local and national governments would be better equipped to justify their road safety policies if the evidence on which they are based is the result of reproducible research conducive to 'citizen science' [@bonney_next_2014]; 
advocacy groups such as RoadPeace would be able to engage not only in lobbying, but also science, encouraging arguments from all sides to be based more on objective evidence, rather than emotive anecdote;
and citizens themselves should benefit, from better road safety policies and the educational opportunities created by open science.

These considerations, and the more mundane observation that dozens of researchers were duplicating effort by cleaning STATS19 data --- the official source of road crash data in Great Britain [@STATS19Data] --- instead of pooling resources to allow the focus to shift onto the research, lead to the development of software written in the statistical programming language R [@rcore]: **stats19**, an R package that was released on the Comprehensive R Archive Network (CRAN) in January 2019 [@lovelace_stats19_2019].

Much road safety research has been done using Geographic Information Systems (GIS) software [e.g. @kim_using_1996; @peled_pc-oriented_1993; @steenberghen_intra-urban_2004;@razzak_application_2011] and, with the growth of open source GIS products such as QGIS, this is a trend that can encourage open science, as defined above.
A limitation of dedicated GIS software products from a reproducibility perspective, however, is that they tend to be based on a graphic user interface (GUI), rather than a command-line interface (CLI).
This has led to many efforts to push geographic research in a more computational directions, under labels such as Geographic Information Science (GIScience), Geographic Data Science, and Geocomputation [@lovelace_geocomputation_2019].

On a practical level, the approach demonstrated in this paper is conducive reproducible research because it uses code to define the geographic analysis steps undertaken.
The use of RMarkdown to generate this paper ensure reproducibility: all main analysis steps are shown in code chunks which re-run each time the document is compiled [@xie_r_2018].
Beyond the high-level aims of evidence-based policy and reproducible research outlined above, this paper has a more specific purpose:
to show that geographic road safety research *can* be reproducible, with an example that presents new findings on the shifting spatial distribution of car-pedestrian crashes at the national level over the last 5 years.


# Set-up and data preparation

The R packages used in this paper can be installed and loaded as follows:

```{r}
pkgs = c(
  "tidyverse",
  "sf",
  "stats19",
  "tmap"
)
```

```{r, eval=FALSE}
install.packages(pkgs)
purrr::map_lgl(pkgs, require, character.only = TRUE)
```

```{r, echo=FALSE, message=FALSE}
setNames(purrr::map_lgl(pkgs, require, character.only = TRUE), pkgs)
```

The following code downloads, formats and combines crash data over the past 5 years as follows:


```{r cached1, cache=TRUE, message=FALSE, eval=FALSE}
y = 2013:2017
a = map_dfr(y, get_stats19, type = "accidents", ask = FALSE)
```

```{r, echo=FALSE, eval=FALSE}
saveRDS(a, "documents/gisruk/a.Rds")
```

```{r, echo=FALSE}
# saveRDS(a_sample, "documents/gisruk/a_sample.Rds")
download.file("https://github.com/Robinlovelace/stats19-gisruk/releases/download/0.0.1/a_sample.Rds", 
"a_sample.Rds")
```

The resulting datasets is large, consisting of more than half a million
<!-- (`r # format(nrow(a), big.mark = ",")`) -->
(691,641)
rows (crash points), with
`r # ncol(a)` 
31
columns [see @lovelace_stats19_2019 for details on the data].
This is easy to work with in-memory on modern computers, however, consuming 1/3 GB of RAM.
These can be converted into a spatial class, defined by the **sf** package [@pebesma_simple_2018-1].
A sample of 1000 is taken and plotted, for demonstration purposes, as follows (see the resulting Figure 1):^[
Note: to save re-running the previous code chunks, the sample dataset can be downloaded from https://github.com/Robinlovelace/stats19-gisruk/releases
]

```{r, eval=FALSE}
a_sf = format_sf(a)
a_sample = a_sf %>% sample_n(1000)
```

```{r uk-plot1, cache=TRUE, fig.cap="Columns of 'a_sample' variable plotted separately on a UK map.", warning=FALSE, fig.height=4}
a_sample = readRDS("a_sample.Rds")
plot(a_sample)
```

Having gained a measure of the crash data, and some key descriptive statistics, we can proceed to join-on the associated casualty and vehicle tables.
The following command uses the argument `type` to specify which table from the STATS19 schema is to be read-in:

```{r, eval=FALSE}
c = map_dfr(y, get_stats19, type = "casualties", ask = FALSE)
v = map_dfr(y, get_stats19, type = "vehicle", ask = FALSE)
```

```{r, echo=FALSE, cache=TRUE, eval=FALSE}
# note: previous line must be done interactively
# saveRDS(c, "documents/gisruk/c.Rds")
# saveRDS(v, "documents/gisruk/v.Rds")
# c = readRDS("documents/gisruk/c.Rds")
# v = readRDS("documents/gisruk/v.Rds")
c = readRDS("c.Rds")
v = readRDS("v.Rds")
```


We are interested in accidents in which a pedestrian was hurt, and where the (only) vehicle involved was a car.
This subset of the casualties dataset can be extracted as follows:

```{r, eval=FALSE}
c_ped = c %>% filter(casualty_type == "Pedestrian")
v_car = v %>% filter(vehicle_type == "Car")
a_cp = a_sf %>%
  filter(number_of_vehicles == 1 & number_of_casualties == 1) %>% 
  filter(accident_index %in% c_ped$accident_index) %>% 
  filter(accident_index %in% v_car$accident_index)
```

```{r, eval=FALSE, echo=FALSE}
nrow(a_cp) / nrow(a)
```

Before proceeding, it's worth joining-on the vehicle and crash tables onto the crash data:
<!-- , keeping only records in which casualty *and* vehicle data is present. -->

```{r, eval=FALSE}
a_cpj = a_cp %>% 
  inner_join(v_car) %>% 
  inner_join(c_ped)
```

```{r, echo=FALSE}
u = "https://github.com/Robinlovelace/stats19-gisruk/releases/download/0.0.1/a_cpj.Rds"
f = "a_cpj.Rds"
if(!file.exists(f)) download.file(url = u, destfile = f)
# file.copy("documets/gisruk/a_cpj.Rds", "a_cpj.Rds")
# piggyback::pb_upload("a_cpj.Rds")
# piggyback::pb_download("a_cpj.Rds")
# file.copy("a_cpj.Rds", "documets/gisruk/a_cpj.Rds")

# saveRDS(a_cpj, "documents/gisruk/a_cpj.Rds")
a_cpj = readRDS("a_cpj.Rds")
# a_cpj = readRDS("documents/gisruk/a_cpj.Rds")
```

The resulting dataset, `a_cpj`, contains 78,454 rows: 11% of the crashes in the original dataset represent a car-pedestrian collision involving a single vehicle and a single casualty (the pedestrian).
This is the dataset, that also contains 68 columns, will be used for the remainder of this analysis.
The final code chunk this section generates a couple of plots, to give us an idea of the nature of car-pedestrian crashes.
As illustrated in Figures 2 and 3, the results match prior expectations: 
elderly people (in the 66-75 and 75+ age bands) and fast roads (40 to 70 miles per hour) tend to result in more serious and fatal injuries.

```{r, echo=FALSE}
a_cpj$impact = a_cpj$first_point_of_impact
a_cpj$impact[grepl(pattern = "missing|not", x = a_cpj$impact)] = "Other"
a_cpj$age_band_of_casualty[a_cpj$age_band_of_casualty == "6 - 10"] = "06 - 10"
```


```{r}
g = ggplot(a_cpj)
```

```{r}
p1 = g + geom_bar(aes(accident_severity, fill = urban_or_rural_area)) +
 facet_wrap(vars(speed_limit), scales = "free_y") +
  labs(fill = "Location")
p2 = g + geom_bar(aes(accident_severity, fill = impact)) +
  facet_wrap(vars(age_band_of_casualty), scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45))
```


```{r, fig.cap="Crash severity by speed limit (top) and crash severity by age band of casualty (bottom)", fig.height=6, echo=FALSE}
devtools::install_github("thomasp85/patchwork")
library(patchwork)
p1 + p2 + plot_layout(ncol = 1, heights = c(4, 5))
```

```{r, eval=FALSE, echo=FALSE}
# base R plotting
barplot(table(a_cpj$accident_severity))

```

\newpage

# Geographic analysis and results

The data is still in a spatial form, of class `sf`, enabling geographic analysis.
Although the geographic resolution of the data is high, ~10 m, we will analyse it at the national level, to investigate the relative performance of different police forces over time.
A geographic join will be used to assign a police force to each crash (although police force is already a variable in the dataset):

```{r}
agg_slight = aggregate(a_cpj["accident_severity"], police_boundaries,
                      function(x) sum(grepl(pattern = "Slight", x)))
```


```{r, echo=FALSE, fig.cap="Overview of crashes by police force area, showing relative numbers of slight, serious and fatal injuries.", message=FALSE}
agg_severe = aggregate(a_cpj["accident_severity"], police_boundaries,
                      function(x) sum(grepl(pattern = "Serious", x)))
agg_fatal = aggregate(a_cpj["accident_severity"], police_boundaries,
                      function(x) sum(grepl(pattern = "Fatal", x)))
agg_none = agg_slight[0]
agg_all = agg_none
agg_all$slight = agg_slight$accident_severity
agg_all$serious = agg_severe$accident_severity
agg_all$fatal = agg_fatal$accident_severity
b = 10^(-1:5)
tm_shape(agg_all) +
  tm_polygons(c("slight", "serious", "fatal"), palette = "viridis", breaks = b)
```

Repeating this process for each crash severity type results in the plot presented in Figure 4.
Because a relative scale is used between the different crash categories, the results shown in Figure 4 shows that, outside London, serious and fatal crashes are comparatively common in some areas.
We can identify which police forces have the highest *ratios* of crashes that are reported as fatal.
The top 5 and bottom 5 are shown in Table 1, which shows wide variability.
As would be expected, large cities (where average speeds tend to be low) tend to have a relatively low percentage of car-pedestrian casualties that are fatal, whereas predominantly rural forces such as Wiltshire and Gloucestershire (where the roads tend to be faster, and there are fewer crashes overall) tend to have a relatively high proportion that are fatal.
Devon and Cornwall is an outlier: a relatively rural force with a low proportion of fatalities.
Further research could seek to explore the reasons for this variability.

```{r, echo=FALSE}
agg_all$name = police_boundaries$pfa16nm
agg_names = agg_all %>% 
  mutate(percent_fatal = fatal / (slight + serious + fatal) * 100)
# mapview::mapview(agg_names) # verification
top_n = agg_names %>% 
  arrange(desc(percent_fatal)) %>% 
  top_n(5, percent_fatal) %>% 
  st_drop_geometry() %>% 
  dplyr::select(name, slight, serious, fatal, percent_fatal)

bottom_n = agg_names %>% 
  arrange(desc(percent_fatal)) %>% 
  top_n(-5, percent_fatal) %>% 
  st_drop_geometry() %>% 
  dplyr::select(name, slight, serious, fatal, percent_fatal)

na_df = matrix(data = NA, nrow = 1, ncol = ncol(bottom_n)) %>% 
  as.data.frame()
names(na_df) = names(bottom_n)

top_bottom = bind_rows(top_n, na_df, bottom_n)
knitr::kable(top_bottom, digits = 1, caption = "Top and bottom 5 police forces in terms of the percentage of car-pedestrian crashes that are fatal.")
```

\newpage

What about variability *over time*?
The overall trend in the number of pestrians hit by cars can be seen in Figure 4, which shows the total number of people by month, broken-down by crash severity.
This result shows that pedestrian casualty rates have essentially flat-lined over the past 5 years, after decades of improvement.
What the data does not show, however, is the geographic breakdown of these trends.

```{r, echo=FALSE, fig.cap="Variability of crash rates over time." }
a_cpj$year = lubridate::year(a_cpj$date)
a_cpj$month = lubridate::month(a_cpj$date)
a_cpj$pct_yr = a_cpj$month / 13
a_cpj$year_month = a_cpj$year + a_cpj$pct_yr
a_time = a_cpj %>% 
  st_drop_geometry() %>% 
  group_by(year_month, casualty_severity) %>% 
  summarise(n = n())
ggplot(a_time) +
  geom_line(aes(year_month, n, lty = casualty_severity)) +
  scale_y_log10()
```

A geographic join can assign each crash to a police authority as follows:

```{r}
a_cps = st_join(a_cpj, police_boundaries)
```

The new object has the variable `pfa16nm`, the police force name, which can be subsequently aggregated and then joined back onto the geographic variable of `police_boundaries`.
Before we plot the 'best' and 'worst' performers geographically, let's see the temporal trend of the top and bottom forces in terms of the percentage of casualties that were fatal (see Table 1).
The results, presented in Figure 5, suggest that London (controlled by the Metropolitan Police) has seen an increase in serious, and to a lesser extent slight and fatal, pedestrian casualties since around the beginning of 2016.
Lancaster has seen an increas in the number of fatalities per month, beginning around the same time.
These raise the question: why?
Rather than answer this question, the final analysis will explore the geographic distribution of improving/worsening performance by crash type.

```{r, echo=FALSE, message=FALSE, fig.cap="Average number of pedestrian casualties by severity and police force in a selection of areas (see Table 1)"}
a_cps$year = lubridate::year(a_cps$date)
a_cps$month = lubridate::month(a_cps$date)
a_cps$pct_yr = a_cps$month / 13
a_cps$year_month = a_cps$year + a_cps$pct_yr
# a_cps$quarter = lubridate::quarter(a_cps$date, with_year = TRUE)
a_cps_sub = a_cps[a_cps$pfa16nm %in% top_bottom$name[7:10], ]
a_time = a_cps_sub %>% 
  st_drop_geometry() %>% 
  group_by(year_month, pfa16nm, casualty_severity) %>% 
  summarise(n = n())
ggplot(a_time) +
  geom_smooth(aes(year_month, n, col = pfa16nm)) +
  facet_wrap(vars(casualty_severity), scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45))
  
```

\newpage

When the analysis presented in Figure 5 is conducted for *all* police jurisdictions, and an aggregate measure of 'direction of trend' is used (in this case, average increase/decrease in counts of crashes of different severity levels per year) is used, we can see the spatial distribution of improvement in casualty numbers across the country (see Figure 6).
It is clear that, assuming crash counts are a good metric of safety (which may not always hold, but is sufficient for the purposes of this paper), some areas perform much better than others.
In terms of fatal car-pedestrian crashes, it is clear that large regions including West Yorkshire, Greater Manchester and Northumbria are not performing well.
The trend for serious car-pedestrian crashes is even more mixed, with London and regions to the east (including Kent and Essex), seeing substantial upward trends in the number of pedestrians sersiously hurt in car crashes.

```{r, echo=FALSE, fig.cap="Trend in car-pedestrian casualties by region, 2013 to 2017, in units of average number of additional casualties per year, by severity of injuries.", message=FALSE}
region = "Lancashire"
sev = "Fatal"
sel = a_cps$pfa16nm == region
a_cps_sub1 = a_cps[sel, ]
a_agg = a_cps %>% 
  st_drop_geometry() %>% 
  group_by(pfa16nm, year) %>% 
  summarise(
    Fatal = sum(casualty_severity == "Fatal"),
    Serious = sum(casualty_severity == "Serious"),
    Slight = sum(casualty_severity == "Slight")
    )
a_cor = a_agg %>% 
  group_by(pfa16nm) %>% 
  summarise(
    Fatal = lm(Fatal ~ year)$coefficients[2],
    Serious = lm(Serious ~ year)$coefficients[2],
    Slight = lm(Slight ~ year)$coefficients[2]
    )
agg_cor = left_join(police_boundaries, a_cor)
a_highlight = filter(police_boundaries, pfa16nm %in% top_bottom$name[7:10])
a_highlight$nm = stringr::str_sub(string = a_highlight$pfa16nm, start = 1, end = 3)
b = c(60, 5, 1, 0)
bb = c(-b, b[3:1])
tm_shape(agg_cor) +
  tm_fill(c("Fatal", "Serious", "Slight"), palette = "-Spectral", alpha = 0.8, breaks = bb) +
  tm_borders() +
  tm_shape(a_highlight) +
  tm_borders(col = "blue", lwd = 2, alpha = 0.4) +
  tm_text("nm") 
  # tm_layout(legend.outside = T)
```

# Discussion

This paper has provided a taster of what is possible with open road crash data, automatically downloaded and formatted using the **stats19** package.
It reveals interesting regional differences in the numbers, proportions and trends of one particular type of road crash: car-pedestrian collisions.
Although the roads are complex systems, and further research should seek to identify suitable denominators of risk (e.g. walking rates), we can draw some conclusions.
The absolute and proportional increase in serious casualties in London is concerning, especially given the government's commitment to contribute to the European Union's target of halving road traffic deaths by 2050.^[
https://fleetworld.co.uk/uk-falling-behind-on-road-safety-targets/
]
The results reflect the overall findings that crash rates, and deaths in particular, have increased in recent years.^[
http://www.brake.org.uk/facts-resources/1653-uk-road-casualties
]
But beyond high-level aggregate analysis, the paper shows how road crash data can be disaggregated in many ways, including by casualty type (e.g. age, gender), time and location.
Although many interesting results have been generated, the truth is that this paper only really scratches the surface of what is possible with the 68 columns and hundreds of thousands of roads of the joined STATS19 data.

This suggests many future areas of research.
From a policy perspective, can automated summary graphics provide insight into performance and early warnings of increases in certain types of crashes?
Can recent findings about the effectiveness of different interventions, particuarly around 20 mph zones and limits [@grundy_effect_2009; @aldred_cycling_2018] be replicated using open data and publicly available code?

From a GIS perspective, the data presented in this paper are undoubtedly of great interest in terms of their size (there are several million points in the open STATS19 data, going back to 1979), richness (with 50+ variables across 3 tables which can be judiciously joined) and spatial resolution (around 10m, although this has not been verified).
This raises further questions about interactive data visualisation, for example using the `geoplumber` package, which builds on `plumber` [@plumber], and the possibility for web applications building on sites such as [www.crashmap.co.uk](https://www.crashmap.co.uk/).
Although more theoretical directions are suggested by the complex processes that result in crashes (point patterns on a linear network), the recommendation from this paper is that future academic work is driven primarily by policy need.

# Acknowldgements

The work was funded by the Leeds Insititute for Transport Studies (ITS) Consumer Data Resarch Center (CDRC) at Leeds Institute for Data Analytics (LIDA).
Thanks to open source software developers who made all this possbile, and to GitHub, where the source code behind this paper is hosted:  https://github.com/Robinlovelace/stats19-gisruk

# References